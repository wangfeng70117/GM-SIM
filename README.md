# GM-SIM
## Processing your own Scenes
### Data preparation
 If you want to prepare masks on your own dataset, you will also need to prepare [DEVA](https://github.com/hkchengrex/Tracking-Anything-with-DEVA) environment.
```
cd submodules
git clone https://github.com/hkchengrex/Tracking-Anything-with-DEVA.git
cd Tracking-Anything-with-DEVA
pip install -e .

cd submodules
git clone https://github.com/hkchengrex/Grounded-Segment-Anything.git
cd Grounded-Segment-Anything
export AM_I_DOCKER=False
export BUILD_WITH_CUDA=True
python -m pip install -e segment_anything
python -m pip install -e GroundingDINO
```


After DEVA is installed, use to generate grayscale segmentation images that are consistent in multiple perspectives.
The dataset and submodule file structure required by the program are as follows:
```
├── data
│   ├──DATASET_NAME
│   │   ├──images
│   │   │   ├──<image 0>
│   │   │   ├──<image 1>
│   │   │   ├──...
├── submodule
│   ├── Grounded-Segment-Anything
│   ├── langsplat-rasterization
│   ├── segment-anything-langsplat
│   ├── simple-knn
│   └── Tracking-Anything-with-DEVA
│   |   ├── demo_automatic.py
|   |   ├── ...
...
```

Then, generate multi-view consistent segmentation images.  
Note that we moved demo_automatic.py from the /Tracking-Anything-with-DEVA/demo folder to /Tracking-Anything-with-DEVA.

```
cd Tracking-Anything-with-DEVA
python demo_automatic.py --chunk_size 4 --img_path "../../data/DATASET_NAME/images" --amp --temporal_setting semionline --size 480 --output "../../data/DATASET_NAME" --use_short_id --suppress_small_objects --SAM_PRED_IOU_THRESHOLD 0.7
```



Generate the camera pose required for 3DGS training, put the images you want to use in a directory. Our COLMAP loaders expect the following dataset structure in the source path location:
```
├── data
│   ├──DATASET_NAME
│   │   ├──images
│   │   │   ├──<image 0>
│   │   │   ├──<image 1>
│   │   │   ├──...
```
Then run
```
python convert.py -s data/DATASET_NAME
```


You can use different methods to generate depth images, such as [Depth Anything V2](https://github.com/DepthAnything/Depth-Anything-V2?tab=readme-ov-file#usage) used by [3DGS](https://github.com/graphdeco-inria/gaussian-splatting), but instead of using a deep learning-based depth prediction model, we use [2d-gaussian-splatting](https://github.com/hbb1/2d-gaussian-splatting) for depth map generation. 2d-gaussian effectively generates a depth map that better matches the shape of the data set.
The rendering depth map is then copied to the dataset folder, The directory structure of dataset required for training is as follows:

```
├── data
│   ├── DATASET_NAME
│   │   ├── Annotations  (Multi-view consistent grayscale map)
│   │   ├── depth_map    (Depth map generated by 2DGS or other methods)
│   │   ├── distorted
│   │   ├── images
│   │   ├── run-colmap-geometric.sh
│   │   ├── run-colmap-photometric.sh
│   │   ├── sparse
│   │   └── stereo
...
```
### Training
- **Step 1: Generate CLIP Feature of the Scenes.**
We use [open-clip](https://github.com/mlfoundations/open_clip) to generate the clip features. Make sure you include the `images` and the `Annotations` folder under `data/DATASET_NAME` to ensure that the same object in different perspectives has consistent CLIP features. And you can download the checkpoints of SAM from [here](https://github.com/facebookresearch/segment-anything#model-checkpoints).
```
python create_clip_feature_maps.py --dataset_name DATASET_NAME --sam_ckpt_path CKPT_PATH
```

---

- **Step 2: Train the encoder and the decoder, Get the 3D language feature**  
Encoder: Encode 512-dimensional CLIP features into 3-dimensional features  
Decoder: decodes 3-dimensional features back to 512-dimensional features  
Train the autoencoder
```
python autoencoder/encoder_decoder.py --dataset_path DATASET_PATH
```
Then get the 3d language feature of the dataset
```
python autoencoder/create_features3d.py --dataset_path DATASET_PATH
```

---

- **Step 3: Train the scene**
```
python train.py -s DATASET_PATH -m DATASET_PATH/output
```

---

- **Step 4: Train 3D language features to the scene**
```
python train.py -s DATASET_PATH -m DATASET_PATH/output --include_feature --start_checkpoint chkpnt_origin.pth --enable_vol_loss
```
If there are too many Gaussians in the scene, volume loss can be deprecated

---

- **Step 5: Use the webui to determine the segmentation parameters**

<img width="1917" height="968" alt="QQ20251112-182812" src="https://github.com/user-attachments/assets/322bfa0b-daaf-40fd-a260-abe13d165371" />


### Basic Controls

- **Reset Scene**  
  Reset the entire scene to its initial state.

- **Prompt**  
  Text prompt for semantic segmentation.

- **Calculate Similarity**  
  Compute the similarity between all Gaussian splats and the given text prompt.


### Segmentation Parameters

- **SegThreshold**  
  The similarity threshold between Gaussian splats and the text prompt.  
  Range: `0–1`.  
  You can switch to explicit point cloud view and determine the threshold range based on color distribution.

- **eps**  
  Parameter for the denoising algorithm used in clustering.  
  The effective `eps` for DBSCAN is calculated as `(scene_size / eps)`.

- **cluster_num**  
  The `min_samples` parameter of the DBSCAN algorithm.

- **grid_n**  
  Defines the number of cells when dividing the bounding box of the segmented point cloud.  
  Each cell contains the Gaussians within that region.


### Object Selection and Segmentation

- **Delete Choice Object**  
  - Checked ✅: Delete the **selected object**.  
  - Unchecked ⬜: Delete the **background** instead.

- **Segment**  
  Perform object segmentation.  
  If the segmentation result is unsatisfactory, click **Reset Scene** to readjust parameters.

  **Tips based on experience:**
  - If too many objects or excessive noise appear → **increase `eps`** or **increase `SegThreshold`**.  
  - If you encounter  
    ```
    IndexError: min(): Expected reduction dim 0 to have non-zero size.
    ```
    it means no Gaussian splats meet the condition — try **decreasing `cluster_num`** or **reducing `SegThreshold`**.

  **Recommended settings:**
  - Large scenes (e.g., *figurines*): `eps = 30`, `SegThreshold = 0.8`  
  - Small scenes: `eps = 5`, `SegThreshold = 0.8`  
  - For very small objects: **reduce `cluster_num`**.


### Ground Alignment and Transformation

- **CalculateGround**  
  Compute the ground plane of the point cloud and rotate the scene to align with the **XOY** plane.

- **Inverse Rotated**  
  Undo the rotation and restore the scene to its original orientation.


### Gaussian Manipulation and Saving

- **Move Segmented Gaussians**  
  After segmentation, obtain the mask of the target Gaussian splats.  
  Use the **XYZ sliders** to move the segmented Gaussians.

- **Save Moved Gaussians**  
  Save the scene after moving the target Gaussians.  
  ⚠️ Please run **Inverse Rotated** before saving.

- **Save Renders**  
  Save the current rendering shown in the web UI.

- **Save Mask**  
  Save the mask of the segmented target object.
---

